Load libraries
```{r}
library(tidyverse)
library(readxl)
library(readr)
library(caTools) # For sample.split function
library(pROC) # For roc/auc functions
library(e1071) # For naive bayes model
library(randomForest) # For random forest model
library(xgboost) # For xgboost model
library(writexl)
```

Load dataframe with information of every throw.
```{r}
throws_info <- read_csv('throws_info.csv')
```

Check the importance of variables in throws_info to determine which are valuable to include in the first base catch probability model (xCR).
```{r}
set.seed(2003)
# Split 60% of thorows to training set and 40% of throws to testing set.
sample <- sample.split(throws_info$was_caught, SplitRatio = 0.6)
data_train <- subset(throws_info, sample == TRUE)
data_test <- subset(throws_info, sample == FALSE)
# See how all these variables affect was_caught
logistic_model <- glm(was_caught ~ Ellipsoid + Ellipsoidx + Ellipsoidy + Ellipsoidz + has_bounce + throw_velocity + throw_distance + throw_duration + at_teamA1 + at_teamA2 + at_teamA3,
                      data_train,
                      family = 'binomial')
summary(logistic_model)
# The statistically significant variables (p-value < 0.05) are Ellipsoid, Ellipsoidx, Ellipsoidy, Ellipsoidz, and has_bounce which aligns with intuition for what would affect the probability of a catch.
```

Select only the variables that will be used in the model based on the above.
```{r}
modelling_data <- throws_info %>%
  select(c(Ellipsoid, Ellipsoidx, Ellipsoidy, Ellipsoidz, has_bounce, was_caught))
```

Logistic model
```{r}
set.seed(2003)
sample <- sample.split(modelling_data$was_caught, SplitRatio = 0.6)
data_train <- subset(modelling_data, sample == TRUE)
data_test <- subset(modelling_data, sample == FALSE)
# Fitting logistic model to training set
logistic_model <- glm(was_caught ~ .,
                      data_train,
                      family = 'binomial')
# Predicting on testing set
logistic_predictions <- predict(logistic_model, data_test, type = 'response')
# Build a confusion matrix to evaluate model
logistic_conf_matrix <- table(factor(ifelse(logistic_predictions >= 0.5, 1, 0)), factor(data_test$was_caught))
logistic_precision <- logistic_conf_matrix[2, 2] / sum(logistic_conf_matrix[, 2])
logistic_recall <- logistic_conf_matrix[2, 2] / sum(logistic_conf_matrix[2, ])
logistic_f1_score <- 2 * (logistic_precision * logistic_recall) / (logistic_precision + logistic_recall)
logistic_roc <- roc(as.numeric(data_test$was_caught), as.numeric(logistic_predictions))
logistic_auc <- auc(logistic_roc)
# Precision = 0.9946, recall = 0.9725, f1 score = 0.9834, auc = 0.593
```

Naive bayes model
```{r}
set.seed(54)
# Fitting naive bayes model to training set
naive_bayes_model <- naiveBayes(was_caught ~ ., data_train)
# Predicting on testing set
bayes_predictions <- predict(naive_bayes_model, data_test, type = 'class')
bayes_conf_matrix <- table(bayes_predictions, data_test$was_caught)
# Build a confusion matrix to evaluate model
bayes_precision <- bayes_conf_matrix[2, 2] / sum(bayes_conf_matrix[, 2])
bayes_recall <- bayes_recall <- bayes_conf_matrix[2, 2] / sum(bayes_conf_matrix[2, ])
bayes_f1_score <- 2 * (bayes_precision * bayes_recall) / (bayes_precision + bayes_recall)
bayes_roc <- roc(as.numeric(data_test$was_caught), as.numeric(bayes_predictions))
bayes_auc <- auc(bayes_roc)
# Precision = 0.9745, recall = 0.9811, f1 score = 0.9778, auc = 0.669
```

Random forest model
```{r}
set.seed(218)
# Random forest model requires target variable to be a factor
rf_modelling_data <- modelling_data %>% 
  mutate(was_caught = as.factor(was_caught))
rf_data_train <- rf_modelling_data[sample,]
rf_data_test <- rf_modelling_data[!sample,]
# Fitting random forest model to training set
random_forest_model <- randomForest(was_caught ~ ., 
                        data = rf_data_train, 
                        ntree = 100,
                        importance = TRUE,
                        proximity = TRUE)
# Predicting on testing set
rf_predictions <- predict(random_forest_model, rf_data_test)
# Build a confusion matrix to evaluate model
rf_conf_matrix <- table(rf_predictions, rf_data_test$was_caught)
rf_precision <- rf_conf_matrix[2, 2] / sum(rf_conf_matrix[, 2])
rf_recall <- rf_conf_matrix[2, 2] / sum(rf_conf_matrix[2, ])
rf_f1_score <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
rf_roc <- roc(as.numeric(rf_data_test$was_caught), as.numeric(rf_predictions))
rf_auc <- auc(rf_roc)
# Precision = 0.9987, recall = 0.9739, f1 score = 0.9861, auc = 0.545
```

XGBoost model
```{r}
set.seed(410)
# xgboost model requires data in matrix form
xgboost_matrix <- modelling_data %>%
  select(!was_caught) %>%
  as.matrix()
# Also it requires the label variable separate from the indicator variables
xgboost_label <- modelling_data$was_caught
xgboost_matrix_train <- xgboost_matrix[sample,]
xgboost_matrix_test <- xgboost_matrix[!sample,]
xgboost_label_train <- xgboost_label[sample]
xgboost_label_test <- xgboost_label[!sample]
# Fitting xgboost model to training set
xgboost_model <- xgboost(data = xgboost_matrix_train, 
                         label = xgboost_label_train, 
                         nrounds = 10,
                         objective = 'binary:logistic')
# Predicting on testing set
xgboost_predictions <- predict(xgboost_model, xgboost_matrix_test, type = 'response')
# Build a confusion matrix to evaluate model
xgboost_conf_matrix <- table(ifelse(xgboost_predictions >= 0.5, 1, 0), xgboost_label_test)
xgboost_precision <- xgboost_conf_matrix[2, 2] / sum(xgboost_conf_matrix[, 2])
xgboost_recall <- xgboost_conf_matrix[2, 2] / sum(xgboost_conf_matrix[2, ])
xgboost_f1_score <- 2 * (xgboost_precision * xgboost_recall) / (xgboost_precision + xgboost_recall)
xgboost_roc <- roc(xgboost_label_test, xgboost_predictions)
xgboost_auc <- auc(xgboost_roc)
# Precision = 0.9933, recall = 0.975, f1 score = 0.9841, auc = 0.812
```

Determine CAA rankings by player.
```{r}
# Make expected catch rate (xCR) predictions using xgboost model for every throw
xCR <- predict(xgboost_model, xgboost_matrix, type = 'response')
throws_info_with_xCR <- cbind(throws_info, xCR) %>%
  distinct(play_id, game_str, first_base, Ellipsoid, Ellipsoidx, Ellipsoidy, Ellipsoidz, has_bounce, was_caught, xCR)
write_csv(throws_info_with_xCR, 'throws_info_with_xCR.csv')
# Create CAA player rankings
player_rankings <- throws_info_with_xCR %>%
  group_by(first_base) %>%
  summarise(CAA = sum(was_caught - xCR)) %>%
  arrange(desc(CAA))
write_csv(player_rankings, 'player_rankings.csv')
```

Create a table with the top 5 and bottom 5 in CAA rankings.
```{r}
top5 <- head(player_rankings, 5)
bottom5 <- tail(player_rankings, 5)
top5_bottom5 <- rbind(top5, bottom5)
write_csv(top5_bottom5, 'top5_bottom5.csv')
```